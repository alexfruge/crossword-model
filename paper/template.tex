%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 2.0 (February 7, 2023)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
% NOTE: The bibliography needs to be compiled using the biber engine.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------


\documentclass[
	a4paper, % Paper size, use either a4paper or letterpaper
	10pt, % Default font size, can also use 11pt or 12pt, although this is not recommended
	unnumberedsections, % Comment to enable section numbering
	twoside, % Two side traditional mode where headers and footers change between odd and even pages, comment this option to make them fixed
]{LTJournalArticle}

\addbibresource{sample.bib} % BibLaTeX bibliography file

\usepackage{amsmath}  % For \mathbf, \text, and math operators
\usepackage{amssymb}  % For \odot

\runninghead{CrosswordLLM} % A shortened article title to appear in the running head, leave this command empty for no running head

\footertext{} % Text to appear in the footer, leave this command empty for no footer text

\setcounter{page}{1} % The page number of the first page, set this to a higher number if the article is to be part of an issue or larger work

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{CrosswordLLM: A Benchmark Framework for Evaluating Large Language Models on Linguistic Reasoning in Crossword Puzzles} % Article title, use manual lines breaks (\\) to beautify the layout

% Authors are listed in a comma-separated list with superscript numbers indicating affiliations
% \thanks{} is used for any text that should be placed in a footnote on the first page, such as the corresponding author's email, journal acceptance dates, a copyright/license notice, keywords, etc
\author{%
	Alex Frugé
}


% Full-width abstract
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
		\noindent I'll fill this out later once I get some results, and finalize any fine-tuning steps I plan on making (embeddings, prompt engineering, etc).
	\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the title section

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

Large Language Models (LLMs) have demonstrated remarkable proficiency in natural language
understanding and generation. However, their ability to solve complex linguistic tasks, such as crosswords, remains an open area of investigation. This project aims to design an evaluation pipeline to test how well different LLMs can solve crossword clues. By analyzing performance variations across model sizes and architectures, I seek to understand whether larger models inherently perform better, or if architectural differences contribute more significantly to success. Additionally, I will explore how contextual information, prompt engineering, and explicit length requirements for answers may affect model performance. This research will not only offer insights into the cognitive capabilities of LLMs but also contribute to broader AI evaluations in natural language processing (NLP).

%------------------------------------------------

\section{Literature Review}

\subsection{Foundational Work}

The sources within the Foundational Work section mainly go discuss how LLMs scale better with more compute, which was one of the attributes I wanted to verify in my crossword case.

\begin{itemize}
	\item Brown et al. (2020) introduced GPT-3, demonstrating that large-scale LLMs can perform a variety of tasks with minimal examples, a technique known as few-shot learning. This work established the potential of LLMs to generalize across tasks without extensive fine-tuning.
	\item Chowdhery et al. (2022) presented PaLM, a 540-billion-parameter model trained using the Pathways system. PaLM achieved state-of-the-art results on numerous benchmarks, highlighting the benefits of scaling model size and training data for improved performance.
\end{itemize}
\subsection{Enhancing Few-Shot Learning and Prompting Techniques}

This section highlights methods for improving language model performance with limited data through better prompt design and reasoning strategies. 

\begin{itemize}
	\item Gao et al. (2020) proposed LM-BFF, a method combining prompt-based fine-tuning with automated prompt generation to improve few-shot learning in smaller models. This approach is particularly relevant for scenarios with limited annotated data, such as specialized crossword clues.
	\item Wei et al. (2022) introduced Chain-of-Thought (CoT) prompting, where models generate intermediate reasoning steps before arriving at an answer. CoT prompting significantly improved performance on complex reasoning tasks, suggesting its applicability to the nuanced reasoning required in crossword solving. I ended up using Chain of Thought prompting as one of my enhancements to the existing models.
	\item Zhou et al. (2023) developed Automatic Prompt Engineer (APE), a system that automatically generates and selects effective prompts. APE outperformed human-crafted prompts on various tasks, indicating the potential for automated prompt optimization in crossword applications.
\end{itemize}


\subsection{Previous Applications to Crossword Solving}

\noindent Saha et al. (2024) demonstrated that current LLMs exhibit significant competence in solving cryptic crossword clues, outperforming previous state-of-the-art results by a factor of 2–3. They developed a search algorithm enabling LLMs to solve full crossword grids, achieving 93\% accuracy on New York Times puzzles. This work underscores the feasibility of applying LLMs to complex language tasks like crosswords

\subsection{Emerging Techniques and Considerations}

\noindent Pattern-Aware Chain-of-Thought Prompting (Zhang et al., 2024) emphasizes the importance of diverse reasoning patterns in prompts to enhance model generalization and robustness. This approach could further improve LLM performance on varied crossword clue types.

\subsection{Future Direction}

Integrating these advancements suggests a multifaceted approach to developing an LLM-based crossword-solving suite:
\begin{itemize}
	\item Model Selection: Utilize large-scale models like PaLM for their superior reasoning capabilities.
	
	\item Few-Shot Learning: Implement LM-BFF techniques to fine-tune models with limited crossword-specific data.
	
	
	\item Reasoning Enhancement: Apply CoT and pattern-aware prompting to improve the model's ability to handle complex clues.
\end{itemize}



By building upon these foundational and contemporary works, the proposed suite aims to advance the capability of LLMs in solving crossword puzzles with high accuracy and efficiency.

%------------------------------------------------



\section{Methodology}

\subsection{Data Sourcing \& Explanation}

The data for this project was pulled from Darin Hawley's New York Times Crossword Clues \& Answers 1993-2021 dataset. (Hawley, 2021) This dataset contains 781,573 entries, each of which come with the date the puzzle containing the clue was published, the clue itself, and the answer. 

The NYT dataset is being used as our training data, as it contains a good spread of clue variety, including some pop-culture related clues, some wordplay, and a wide variety of difficulty in the clues themselves. It also contains a good number of clues involving multiple words in a clue being concatenated into one continuous word (see Table \ref{tab:NYTClues} for some examples of this). I'll also use the NYT dataset for a testing suite that tests the selected models on more conventional clues.

\begin{table}[h] % Single column table
	\centering
	\begin{tabular}{c c}
		\toprule
		Answer & Clue \\
		\midrule
		EATAT & Irritate\\
		ALQAEDA & War on terror target\\
		ASGOODASGOLD & 100\% reliable\\
		\bottomrule
	\end{tabular}
	\caption{Example clues from NYT Crossword Clues.}
	\label{tab:NYTClues}
\end{table}

\subsection{Libraries and Tools Used}

The core libraries used within this package are \textbf{PyTorch 2.0} (training original models), \textbf{TransformerLens} (loading original models), \textbf{HuggingFace Transformers} (loading tokenizers), as well as \textbf{Pandas} and \textbf{Scikit-learn}.

\noindent The system leverages the \textbf{Pythia} suite of autoregressive language models (Biderman et al., 2023) as its primary base architecture due to their transparency, reproducibility, and controlled scaling properties. Pythia models provide a standardized framework for experimentation, with versions ranging from 70M to 12B parameters, allowing systematic comparisons across model sizes. For the sake of the models being usable on my computer, the largest model I tested with contained 1.3 billion parameters.

\subsection{Training Process}

The training process follows a structured pipeline designed to optimize the model’s ability to generate accurate crossword answers while incorporating task-specific enhancements. Below is a high-level overview of the training workflow.

The system ingests a structured dataset of (clue, answer, length) tuples, derived from the NYT Crossword Clue Dataset. The clues are then standardized (removing metadata like enumeration), and answers are normalized to uppercase alphabetic strings. Answer lengths are explicitly annotated, either as single integers (for single-word answers) or comma-separated values (for multi-word answers). Data is partitioned into training (80\%) and evaluation (20\%) sets, ensuring no overlap.

Clues are formatted into instructional prompts (e.g., "Solve the crossword clue: [CLUE] ([LENGTH]). Answer:") to guide the model’s generation. Multiple prompting strategies (basic, detailed, chain-of-thought) are explored to improve task alignment. Inputs are tokenized using the base model’s pretrained tokenizer, with padding/truncation to a fixed sequence length.

Once all of the data is brought in and cleaned up, the user specifies which model they want to use as a base, as well as which enhancement they want to add to that base via a CLI (e.g. main.py --model EleutherAI/pythia-70m --enhancement length\_aware would train the pythia-70m model with the length aware enhancement added on).

The model is trained via causal language modeling, where it predicts the answer tokens autoregressively. The loss is computed only on answer tokens (excluding the prompt). Depending on which enhancements the user specifies, there could be intervention here by said enhancements.

The training loop itself uses  the AdamW optimizer with a learning rate of $5E-05$, batch size of 32, and gradient clipping, as well as dropout (where applicable) and early stopping to prevent overfitting.

\subsection{Enhancements}

The base autoregressive language model is augmented with several task-specific enhancements designed to improve its crossword-solving capabilities. These modifications target key challenges in clue-answer generation, including length conditioning, semantic grounding, and controlled decoding.

\subsubsection{Contextual Embedding Fusion}
LMs may lack deep semantic understanding of clues (e.g., parsing wordplay like "Composer of 'The Planets' (5)" → "HOLST").

The solution that I came up with is a hybrid architecture combining a \textbf{frozen sentence embedder} and \textbf{gated fusion}.

The frozen sentence embedder is derived from a pretrained model (e.g., all-MiniLM-L6-v2) encodes clues into dense vectors, capturing synonym relationships ("composer" → "musician") and entity linkages ("The Planets" → "Gustav Holst").

The LM’s hidden states are combined with projected embeddings via:

\[\mathbf{h}_{\text{fused}} = \sigma\left(\mathbf{W}_g \left[\mathbf{h}_{\text{LM}}; \mathbf{h}_{\text{emb}}\right]\right) \odot \mathbf{h}_{\text{LM}} +\]
\[ \left(1 - \sigma\left(\mathbf{W}_g \left[\mathbf{h}_{\text{LM}}; \mathbf{h}_{\text{emb}}\right]\right)\right) \odot \mathbf{h}_{\text{emb}}\]

where $\mathbf{W}_g$ is a learned gate matrix.

\subsubsection{Prompt Engineering Strategies}

The final problem I considered was how clues can require different reasoning modes (definition, wordplay, trivia). I explored using different formatted prompts to guide the LM’s behavior. In the end, I came up with three different prompt formats which each have a different target (detailed in Table \ref{tab:prompt_strategies}).

\begin{table*}[h]
	\centering
	\caption{Prompt Engineering Strategies for Crossword Solving}
	\label{tab:prompt_strategies}
	\begin{tabular}{lp{8cm}l}	
		\hline
		\textbf{Strategy} & \textbf{Example Prompt Structure} & \textbf{Use Case} \\
		\hline
		\textbf{Basic} & \texttt{"Clue: [CLUE] ([LENGTH]). Answer:"} & Simple, definitional clues \\
		\hline
		\textbf{Detailed} & \texttt{"Task: Solve this clue. Clue: `[CLUE]'. Answer must be [LENGTH] letters, no spaces. Answer:"} & Complex clues \\
		\hline
		\textbf{Chain-of-Thought} & \texttt{"Analyze the clue `[CLUE]'. Consider: 1) Definitions 2) Wordplay. Then give a [LENGTH]-letter answer:"} & Wordplay/metaphorical clues \\
		\hline
	\end{tabular}
\end{table*}

\subsection{Testing Process}

The evaluation framework rigorously assesses model performance across multiple dimensions, combining automated metrics with human analysis to ensure comprehensive validation of crossword-solving capabilities.

\begin{table}[t]
	\centering
	\caption{Comprehensive Evaluation Metrics for Crossword-Solving Models}
	\label{tab:evaluation_metrics}
	\begin{tabular}{l l l}
		\toprule
		\textbf{Metric Category} & \textbf{Description} & \textbf{Interpretation} \\
		\midrule
		\textbf{Exact Match Accuracy (EMA)} & \% of answers which are correct & 
		Higher is better \\
		\textbf{Length-Adjusted Accuracy (LAA)} & 
		\% of answers with correct length & 
		Meets crossword grid limits \\
		
	\end{tabular}
	\vspace{0.2cm}
	\footnotesize \textit{Note.} All metrics computed on held-out test set with $n=1000$ samples. Embedding similarity uses \texttt{all-MiniLM-L6-v2}.
\end{table}

%------------------------------------------------

\section{Results}

The first test I ran was running all of the models I selected (pythia-14m through pythia-1.3b) with no enhancements, with the goal of verifying that models increase performance as their size increases.

The results in table \ref{tab:pythiarun1} provide insight into how the performance of models in the Pythia suite scales with size when solving a task based on crossword clues from the New York Times dataset. Here's a summary and interpretation of the findings:

\begin{table}[h] % Single column table
	\centering
	\begin{tabular}{l c c}
		\toprule
		Model & EMA & LMA \\
		\midrule
		pythia-14m & 0.0000 & 0.0320 \\
		pythia-31m & 0.0000 & 0.2580 \\
		pythia-70m & 0.0000 & 0.1210 \\
		pythia-160m & 0.0030 & 0.2150 \\
		pythia-410m & 0.0230 & 0.2400 \\
		pythia-1.3b & 0.0270 & 0.1970 \\
		\bottomrule
	\end{tabular}
	\label{tab:pythiarun1}
	\caption{First run across Pythia Suite using NYT Data (n=12,800, batch size=32, epochs=5)}
\end{table}

As shown by the results, there is a clear trend showing that both Accuracy and Length Match Accuracy generally improve as model size increases. For example, EMA starts at 0.0000 for the smallest models (pythia-14m, 31m, and 70m), and then slowly begins to increase, reaching a max at 0.0270 for pythia-1.3b. LMA follows a similar upward trend, with the pythia-31m surprisingly achieving the highest score here (0.2580), though results fluctuate slightly across models. My guess for this is that the model is getting lucky with the correct length once the model is large enough, since the LMAs hover around 0.2 once pythia-31m and larger are tested.

This suggests that model scale correlates positively with performance, though the relationship is not strictly linear. Small models struggle significantly, likely due to a lack of capacity to represent the complex relationships required to solve crossword clues—a task that typically relies on semantic associations, cultural knowledge, and language nuance.

The fact that EMA remains low even for the larger models (only 2.7\% for pythia-1.3b) indicates that this is a very difficult task, likely involving not just language understanding but also lateral thinking, wordplay, and perhaps common-sense reasoning. The low scores suggest that these models may not yet be well-equipped for this form of symbolic reasoning or require more targeted training.

The Length Match Accuracy is a more forgiving metric, it simply measures whether the model's output matches the length of the correct answer. This improves more noticeably with model size, showing that larger models are better at mimicking superficial features of the answer even if they can't yet solve the clue. The pythia-31m’s unusually high score on this metric may be an outlier or a quirk of training dynamics.


\begin{table}[h] % Single column table
	\centering
	\begin{tabular}{l c c}
		\toprule
		Model & Accuracy & Length Match Accuracy \\
		\midrule
		pythia-14m & 0.0000 & 0.0000 \\
		pythia-31m & 0.0000 & 0.0000 \\
		pythia-70m & 0.0000 & 0.1210 \\
		pythia-160m & 0.0000 & 0.2150 \\
		pythia-410m & 0.0000 & 0.0090\\
		\bottomrule
	\end{tabular}
	\label{tab:embeddingtest}
	\caption{Testing models with Embedding enhancement using NYT Data (n=12,800, batch size=32, epochs=5)}
\end{table}

\begin{table*}[h] % Single column table
	\centering
	\begin{tabular}{l c c}
		\toprule
		Clue & Generated Answer & Actual \\
		\midrule
		Carrie in "Sex and the City" (8) & \textit{no answer} & BRADSHAW \\
		Get hitched to (3)& WED & ANS \\
		Attempt to block (6) & OPPOSE & ANSWER \\
		Hookups for a camera? (8) & SEXTAPES & ANSWER \\
		\bottomrule
	\end{tabular}
	\label{tab:embeddingtestexamples}
	\caption{Example outputs from pythia-70m with Embedding enhancement using NYT Data (n=12,800, batch size=32, epochs=5)}
\end{table*}


67. Clue: Get hitched to (3)
Correct Answer:   WED
Generated Answer: ANS

68. Clue: Attempt to block (6)
Correct Answer:   OPPOSE
Generated Answer: ANSWER

69. Clue: Hookups for a camera? (8)
Correct Answer:   SEXTAPES
Generated Answer: ANSWER

%------------------------------------------------

\section{Conclusion}


%----------------------------------------------------------------------------------------
%	 REFERENCES
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------

\section{References \& AI Use}

https://claude.ai/share/33fa78db-2dc8-4fef-a80b-baa16c511aba

\end{document}
