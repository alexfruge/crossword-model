import os
import torch
import pandas as pd
from sklearn.metrics import accuracy_score
from transformers import GPT2Tokenizer, AutoTokenizer

import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from model import load
from utils import log_statement
from main import generate_answer

# Paths
models_dir = "trained_models"
test_data_path = "data/test.csv"

# Load test data
def load_test_data(file_path, max_elements=1_000):
    """
    Load test data from a CSV file and return specific columns.
    Args:
        file_path (str): The path to the CSV file containing the test data.
        max_elements (int, optional): The maximum number of rows to load from the file. Defaults to 10,000.
    Returns:
        tuple: A tuple containing three pandas Series:
            - clue (pd.Series): The 'clue' column from the CSV file.
            - answer (pd.Series): The 'answer' column from the CSV file.
            - ans_length (pd.Series): The 'ans_length' column from the CSV file.
    Raises:
        ValueError: If the CSV file does not contain the required columns: 'clue', 'answer', and 'ans_length'.
    """

    data = pd.read_csv(file_path)
    if 'clue' not in data.columns or 'answer' not in data.columns or 'ans_length' not in data.columns:
        raise ValueError("Test data must have 'input' and 'output' columns.")
    data=data.head(max_elements)
    return data['clue'], data['answer'], data['ans_length']


# Evaluate model
def evaluate_model(model, inputs, model_name):
    """
    Evaluates a given model by generating predictions for a list of input texts.
    Args:
        model (torch.nn.Module): The model to be evaluated.
        inputs (list of str): A list of input texts for which predictions will be generated.
        model_name (str): The name of the model, used to determine the appropriate tokenizer.
    Returns:
        list of str: A list of predictions generated by the model for the given inputs.
    Notes:
        - If the model name contains "gpt2" (case-insensitive), a GPT-2 tokenizer is used.
        - Otherwise, an AutoTokenizer is used to load the tokenizer based on the model name.
        - Each prediction is logged with a statement indicating the generated prediction.
    """

    if "gpt2" in model_name.lower():
        tokenizer = GPT2Tokenizer.from_pretrained(model_name.split("/")[-1])
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    predictions = []
    for idx, input_text in enumerate(inputs):
        with torch.no_grad():
            prediction = generate_answer(model, tokenizer, input_text)
            predictions.append(prediction)
            if (idx + 1) % 100 == 0:
                log_statement(statement=f"[Predictions] Processed {idx + 1}/{len(inputs)} predictions.", model_name=model_name)
    return predictions

# Metrics
def calculate_metrics(predictions, true_outputs):
    """
    Calculates evaluation metrics for model predictions compared to true outputs.
    Args:
        predictions (list): A list of predicted outputs.
        true_outputs (list): A list of true outputs to compare against.
    Returns:
        tuple: A tuple containing:
            - accuracy (float): The proportion of predictions that exactly match the true outputs.
            - length_match_accuracy (float): The proportion of predictions where the character length matches the true outputs.
    """

    # Convert predictions to strings if necessary
    predictions = [str(pred) for pred in predictions]
    true_outputs = [str(true) for true in true_outputs]

    # Accuracy
    accuracy = accuracy_score(true_outputs, predictions)

    # Character length match
    length_matches = [len(pred) == len(true) for pred, true in zip(predictions, true_outputs)]
    length_match_accuracy = sum(length_matches) / len(length_matches)

    return accuracy, length_match_accuracy

# Main testing function
def test_models():
    """
    Tests multiple models by evaluating their performance on a test dataset.
    This function loads test data and iterates through all model files in the 
    specified directory. For each model, it loads the model, evaluates it on 
    the test inputs, and calculates accuracy metrics. The results are printed 
    to the console.
    Steps:
    1. Load test data including inputs, true outputs, and their lengths.
    2. Identify all model files in the directory with a '.pt' extension.
    3. For each model file:
        - Extract the model name from the file name.
        - Adjust the model name if it corresponds to a specific naming convention (e.g., "pythia").
        - Load the model using the extracted name and file path.
        - Evaluate the model on the test inputs.
        - Calculate accuracy metrics including overall accuracy and character length match accuracy.
        - Print the results.
    Prints:
        - Accuracy of the model on the test dataset.
        - Character length match accuracy of the predictions.
        - Separator line for readability.
    Note:
        - The function assumes the existence of helper functions such as 
          `load_test_data`, `load.load_finetuned_model`, `evaluate_model`, 
          and `calculate_metrics`.
        - The variables `test_data_path` and `models_dir` should be defined 
          and point to the appropriate paths for test data and model files.
    """

    inputs, true_outputs, lengths = load_test_data(test_data_path)
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pt')]

    for model_file in model_files:
        model_name = model_file.replace("model-", "").replace(".pt", "").replace("trained_models/", "")
        if "pythia" in model_name:
            model_name = f"EleutherAI/{model_name}"
        model_path = os.path.join(models_dir, model_file)
        model = load.load_finetuned_model(model_name=model_name, weights_path=model_path)

        predictions = evaluate_model(model, inputs, model_name)
        accuracy, length_match_accuracy = calculate_metrics(predictions, true_outputs)

        print(f"Accuracy: {accuracy:.4f}")
        print(f"Character Length Match Accuracy: {length_match_accuracy:.4f}")
        print("-" * 50)

if __name__ == "__main__":
    test_models()