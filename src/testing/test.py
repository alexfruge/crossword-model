import os
import torch
import pandas as pd
from sklearn.metrics import accuracy_score
from transformers import GPT2Tokenizer, AutoTokenizer
from transformers.utils import logging

import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from model import load
from utils import log_statement
from main import generate_answer_enhanced
from enhancements.contextual_embeddings import create_enhanced_model
from enhancements.prompt_engineering import create_enhanced_dataloader

# Paths
raw_models_dir = "trained_models/raw"
enhanced_models_dir = "trained_models/enhanced"
test_data_path = "data/test.csv"

# Load test data
def load_test_data(file_path, max_elements=1_000):
    """
    Load test data from a CSV file and return specific columns.
    Args:
        file_path (str): The path to the CSV file containing the test data.
        max_elements (int, optional): The maximum number of rows to load from the file. Defaults to 10,000.
    Returns:
        tuple: A tuple containing three pandas Series:
            - clue (pd.Series): The 'clue' column from the CSV file.
            - answer (pd.Series): The 'answer' column from the CSV file.
            - ans_length (pd.Series): The 'ans_length' column from the CSV file.
    Raises:
        ValueError: If the CSV file does not contain the required columns: 'clue', 'answer', and 'ans_length'.
    """

    data = pd.read_csv(file_path)
    if 'clue' not in data.columns or 'answer' not in data.columns or 'ans_length' not in data.columns:
        raise ValueError("Test data must have 'input' and 'output' columns.")
    data=data.head(max_elements)
    return data['clue'], data['answer'], data['ans_length']


# Evaluate model
def evaluate_model(model, inputs, model_name, enhancement=None):
    """
    Evaluates a given model by generating predictions for a list of input texts.
    Args:
        model (torch.nn.Module): The model to be evaluated.
        inputs (list of str): A list of input texts for which predictions will be generated.
        model_name (str): The name of the model, used to determine the appropriate tokenizer.
        enhancement (str, optional): The type of enhancement applied to the model (e.g., "embeddings", "length_aware").
    Returns:
        list of str: A list of predictions generated by the model for the given inputs.
    Notes:
        - If the model name contains "gpt2" (case-insensitive), a GPT-2 tokenizer is used.
        - Otherwise, an AutoTokenizer is used to load the tokenizer based on the model name.
        - Each prediction is logged with a statement indicating the generated prediction.
    """

    if "gpt2" in model_name.lower():
        tokenizer = GPT2Tokenizer.from_pretrained(model_name.split("/")[-1])
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    predictions = []
    for idx, input_text in enumerate(inputs):
        with torch.no_grad():
            prediction = generate_answer_enhanced(model, tokenizer, input_text)
            predictions.append(prediction)
            if (idx + 1) % 100 == 0:
                log_statement(statement=f"[Predictions] Processed {idx + 1}/{len(inputs)} predictions.", model_name=model_name, enhancement=enhancement)
    return predictions

# Metrics
def calculate_metrics(predictions, true_outputs):
    """
    Calculates evaluation metrics for model predictions compared to true outputs.
    Args:
        predictions (list): A list of predicted outputs.
        true_outputs (list): A list of true outputs to compare against.
    Returns:
        tuple: A tuple containing:
            - accuracy (float): The proportion of predictions that exactly match the true outputs.
            - length_match_accuracy (float): The proportion of predictions where the character length matches the true outputs.
    """

    # Convert predictions to strings if necessary
    predictions = [str(pred) for pred in predictions]
    true_outputs = [str(true) for true in true_outputs]

    # Accuracy
    accuracy = accuracy_score(true_outputs, predictions)

    # Character length match
    length_matches = [len(pred) == len(true) for pred, true in zip(predictions, true_outputs)]
    length_match_accuracy = sum(length_matches) / len(length_matches)

    return accuracy, length_match_accuracy

# Main testing function
def test_models():
    """
    Tests multiple models by evaluating their performance on a test dataset.
    """
    inputs, true_outputs, lengths = load_test_data(test_data_path)
    model_files = [f for f in os.listdir(raw_models_dir) if f.endswith('.pt')]
    model_files.extend([f for f in os.listdir(enhanced_models_dir) if f.endswith('.pt')])

    for model_file in model_files:
        model_name = model_file.replace(
            "model-", "").replace(
            ".pt", "").replace(
            "trained_models/", "").replace(
            "raw/", "").replace(
            "enhanced/", "")

        cont = input(f"Test {model_name}? (y/n): ")
        if cont.lower() != 'y':
            continue
        
        if "pythia" in model_name:
            model_name = f"EleutherAI/{model_name}"
        
        if "embeddings" in model_name:
            model_path = os.path.join(enhanced_models_dir, model_file)
            enhancement = "embeddings"
            model = create_enhanced_model(model_name.replace("-embeddings", ""))
        elif "prompt" in model_name:
            model_path = os.path.join(enhanced_models_dir, model_file)
            enhancement = "prompt"
            model = load.load_finetuned_model(model_name=model_name, weights_path=model_path)
        else:
            model_path = os.path.join(raw_models_dir, model_file)
            model = load.load_finetuned_model(model_name=model_name, weights_path=model_path)
        
        # Load the state dictionary
        model.load_state_dict(torch.load(model_path))
        model.eval()

        predictions = evaluate_model(model, inputs, model_name.replace("-embeddings", "").replace("-length_aware", "").replace("-prompt", ""), enhancement=enhancement)
        accuracy, length_match_accuracy = calculate_metrics(predictions, true_outputs)

        log_statement(statement=f"Accuracy: {accuracy:.4f}", model_name=model_name, enhancement=enhancement)
        log_statement(statement=f"Character Length Match Accuracy: {length_match_accuracy:.4f}", model_name=model_name, enhancement=enhancement)
        log_statement(statement="-" * 50, model_name=model_name, enhancement=enhancement)

if __name__ == "__main__":
    # Turn off transformers logging
    logging.disable_progress_bar() # disable INFO and DEBUG logging everywhere
    test_models()